{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2451b503-8b08-4cea-ae6f-c82d7899d3dd",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Implement convolution operation for a sample image of shape (H=6, W=6, C=1) with a\n",
    "random kernel of size (3,3) using torch.nn.functional.conv2d. \n",
    "\n",
    "What is the dimension of the output image? Apply, various values for parameter stride=1 and note the change in the dimension of the output image. Arrive at an equation for the output image size with respect to the kernel size and stride and verify your answer with code. Now, repeat the exercise by changing padding parameter. Obtain a formula using kernel, stride, and padding to get the output image size. What is the total number of parameters in your network? Verify with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f63beb18-756e-4d29-8b0c-b9573e28f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image= tensor([[0.1186, 0.7337, 0.9362, 0.6941, 0.9227, 0.1054],\n",
      "        [0.0437, 0.4127, 0.3661, 0.6174, 0.4510, 0.1018],\n",
      "        [0.0351, 0.9308, 0.9723, 0.7033, 0.8682, 0.1331],\n",
      "        [0.3270, 0.2197, 0.3275, 0.5457, 0.7827, 0.8512],\n",
      "        [0.8301, 0.7932, 0.5169, 0.0567, 0.7926, 0.9195],\n",
      "        [0.8814, 0.2048, 0.5625, 0.8749, 0.1683, 0.0961]])\n",
      "image.shape= torch.Size([1, 6, 6])\n",
      "image.shape= torch.Size([1, 1, 6, 6])\n",
      "image= tensor([[[[0.1186, 0.7337, 0.9362, 0.6941, 0.9227, 0.1054],\n",
      "          [0.0437, 0.4127, 0.3661, 0.6174, 0.4510, 0.1018],\n",
      "          [0.0351, 0.9308, 0.9723, 0.7033, 0.8682, 0.1331],\n",
      "          [0.3270, 0.2197, 0.3275, 0.5457, 0.7827, 0.8512],\n",
      "          [0.8301, 0.7932, 0.5169, 0.0567, 0.7926, 0.9195],\n",
      "          [0.8814, 0.2048, 0.5625, 0.8749, 0.1683, 0.0961]]]])\n",
      "kernel= tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "outimage= tensor([[[[4.5493, 6.3667, 6.5314, 4.5971],\n",
      "          [3.6350, 5.0957, 5.6344, 5.0546],\n",
      "          [4.9527, 5.0661, 5.5659, 5.6531],\n",
      "          [4.6631, 4.1019, 4.6277, 5.0876]]]])\n",
      "Dimension of output image S-1 P-0:  torch.Size([1, 1, 4, 4])\n",
      "Manually dim of output S-1 P-0:  [1, 1, 4, 4]\n",
      "Dimension of output image S-1 P-1: torch.Size([1, 1, 6, 6])\n",
      "Manually dim of output S-1 P-1:  [3, 3, 6, 6]\n",
      "Dimension of output image S-1 P-2: torch.Size([1, 1, 8, 8])\n",
      "Manually dim of output S-1 P-2:  [5, 5, 8, 8]\n",
      "Dimension of output image S-2 P-1:  torch.Size([1, 1, 3, 3])\n",
      "Manually dim of output S-2 P-1:  [2, 2, 3, 3]\n",
      "Dimension of output image S-2 P-1: torch.Size([1, 1, 2, 2])\n",
      "Manually dim of output S-3 P-1:  [1, 1, 2, 2]\n",
      "Number of Learnable Parameters = 9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "image = torch.rand(6,6)\n",
    "print(\"image=\", image)\n",
    "#Add a new dimension along 0th dimension\n",
    "#i.e. (6,6) becomes (1,6,6). This is because\n",
    "#pytorch expects the input to conv2D as 4d tensor\n",
    "image = image.unsqueeze(dim=0)\n",
    "print(\"image.shape=\", image.shape)\n",
    "image = image.unsqueeze(dim=0)\n",
    "print(\"image.shape=\", image.shape)\n",
    "print(\"image=\", image)\n",
    "kernel = torch.ones(3,3)\n",
    "#kernel = torch.rand(3,3)\n",
    "print(\"kernel=\", kernel)\n",
    "kernel = kernel.unsqueeze(dim=0)\n",
    "kernel = kernel.unsqueeze(dim=0)\n",
    "\n",
    "def out_dim(in_shape,stride,padding,kernel_shape):\n",
    "    out_shape = [0 for i in range(4)]\n",
    "    for dim in range(len(in_shape)):\n",
    "        out_shape[dim] = (in_shape[dim] + 2*padding - kernel_shape[dim])//stride + 1\n",
    "    return out_shape\n",
    "    \n",
    "#Stride 1 Padding 0\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=0)\n",
    "print(\"outimage=\", outimage)\n",
    "print(\"Dimension of output image S-1 P-0: \",outimage.shape)\n",
    "print(\"Manually dim of output S-1 P-0: \",out_dim(image.shape,1,0,kernel.shape))\n",
    "\n",
    "#Stride 1 Padding 1\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=1)\n",
    "print(\"Dimension of output image S-1 P-1:\",outimage.shape)\n",
    "print(\"Manually dim of output S-1 P-1: \",out_dim(image.shape,1,1,kernel.shape))\n",
    "\n",
    "#Stride 1 Padding 2\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=2)\n",
    "print(\"Dimension of output image S-1 P-2:\",outimage.shape)\n",
    "print(\"Manually dim of output S-1 P-2: \",out_dim(image.shape,1,2,kernel.shape))\n",
    "\n",
    "#Stride 2 Padding 1\n",
    "outimage = F.conv2d(image, kernel, stride=2, padding=1)\n",
    "print(\"Dimension of output image S-2 P-1: \",outimage.shape)\n",
    "print(\"Manually dim of output S-2 P-1: \",out_dim(image.shape,2,1,kernel.shape))\n",
    "\n",
    "#Stride 3 Padding 1\n",
    "outimage = F.conv2d(image, kernel, stride=3, padding=1)\n",
    "print(\"Dimension of output image S-2 P-1:\",outimage.shape)\n",
    "print(\"Manually dim of output S-3 P-1: \",out_dim(image.shape,3,1,kernel.shape))\n",
    "\n",
    "print(\"Number of Learnable Parameters = 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eced408-fccf-4442-8cb5-a40b2b86f7c6",
   "metadata": {},
   "source": [
    "## Q2 \n",
    "\n",
    "Apply torch.nn.Conv2d to the input image of Qn 1 with out-channel=3 and observe the\n",
    "output. Implement the equivalent of torch.nn.Conv2d using the torch.nn.functional.conv2D\n",
    "to get the same output. You may ignore bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94b074b7-75ad-4ae6-bb2d-10f72b6a15d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel parameters for 3 channels: \n",
      "Parameter containing:\n",
      "tensor([[[[-0.0442, -0.0062,  0.1765],\n",
      "          [ 0.0043, -0.3088,  0.3210],\n",
      "          [ 0.1142, -0.2003, -0.3295]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2522, -0.1952,  0.2530],\n",
      "          [ 0.0576, -0.2026, -0.1792],\n",
      "          [-0.0216,  0.1752,  0.1663]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1334,  0.2059, -0.1624],\n",
      "          [ 0.0182, -0.1681,  0.0662],\n",
      "          [-0.0531, -0.0890, -0.0859]]]], requires_grad=True)\n",
      "Output image using torch.nn.Conv2d: \n",
      "tensor([[[[-0.3563, -0.1274, -0.2278, -0.2432],\n",
      "          [-0.0432, -0.1694, -0.3169, -0.1089],\n",
      "          [ 0.0831,  0.0602, -0.1804,  0.0947],\n",
      "          [-0.2690, -0.1873, -0.1555, -0.0782]],\n",
      "\n",
      "         [[ 0.0793,  0.3803,  0.1059,  0.0877],\n",
      "          [ 0.0911,  0.2746, -0.0041,  0.3246],\n",
      "          [ 0.2784, -0.0466,  0.0713,  0.2349],\n",
      "          [ 0.3512,  0.3800,  0.2487,  0.3445]],\n",
      "\n",
      "         [[-0.0861,  0.0829, -0.2119, -0.0823],\n",
      "          [ 0.1535, -0.2789, -0.1432, -0.0407],\n",
      "          [-0.0901, -0.0240,  0.0776, -0.0866],\n",
      "          [-0.1294, -0.1112, -0.0775, -0.1246]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "Output image using torch.nn.functional.conv2d: \n",
      "tensor([[[[-0.3563, -0.1274, -0.2278, -0.2432],\n",
      "          [-0.0432, -0.1694, -0.3169, -0.1089],\n",
      "          [ 0.0831,  0.0602, -0.1804,  0.0947],\n",
      "          [-0.2690, -0.1873, -0.1555, -0.0782]],\n",
      "\n",
      "         [[ 0.0793,  0.3803,  0.1059,  0.0877],\n",
      "          [ 0.0911,  0.2746, -0.0041,  0.3246],\n",
      "          [ 0.2784, -0.0466,  0.0713,  0.2349],\n",
      "          [ 0.3512,  0.3800,  0.2487,  0.3445]],\n",
      "\n",
      "         [[-0.0861,  0.0829, -0.2119, -0.0823],\n",
      "          [ 0.1535, -0.2789, -0.1432, -0.0407],\n",
      "          [-0.0901, -0.0240,  0.0776, -0.0866],\n",
      "          [-0.1294, -0.1112, -0.0775, -0.1246]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "image= torch.tensor([[[[0.2557, 0.9236, 0.4913, 0.3200, 0.4958, 0.2214],\n",
    "          [0.7554, 0.6501, 0.0107, 0.8675, 0.5163, 0.6102],\n",
    "          [0.8228, 0.1919, 0.8724, 0.8043, 0.3882, 0.9689],\n",
    "          [0.4894, 0.5116, 0.5624, 0.6949, 0.6289, 0.9802],\n",
    "          [0.3913, 0.2773, 0.1427, 0.3717, 0.4154, 0.3669],\n",
    "          [0.8327, 0.8157, 0.7192, 0.9387, 0.4569, 0.6776]]]])\n",
    "\n",
    "conv = nn.Conv2d(in_channels=1,out_channels=3,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "print(\"Kernel parameters for 3 channels: \")\n",
    "kernel = conv.weight\n",
    "print(conv.weight)\n",
    "print(\"Output image using torch.nn.Conv2d: \")\n",
    "out_image = print(conv(image))\n",
    "\n",
    "import torch.nn.functional as F\n",
    "out_image = F.conv2d(image,kernel,stride=1,padding=0)\n",
    "print(\"Output image using torch.nn.functional.conv2d: \")\n",
    "print(out_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e426921-6471-48ed-b498-9017accd3932",
   "metadata": {},
   "source": [
    "## Q3 \n",
    "\n",
    "Implement CNN for classifying digits in MNIST dataset using PyTorch. Display the classification accuracy in the form of a Confusion matrix. Verify the number of learnable parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffe4dd0e-3792-417b-ab9c-c356feefb3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.310\n",
      "[1,   200] loss: 2.297\n",
      "[1,   300] loss: 2.285\n",
      "[1,   400] loss: 2.272\n",
      "[1,   500] loss: 2.249\n",
      "[1,   600] loss: 2.198\n",
      "[1,   700] loss: 2.087\n",
      "[1,   800] loss: 1.823\n",
      "[1,   900] loss: 1.423\n",
      "[1,  1000] loss: 1.160\n",
      "[1,  1100] loss: 0.925\n",
      "[1,  1200] loss: 0.799\n",
      "[2,   100] loss: 0.698\n",
      "[2,   200] loss: 0.603\n",
      "[2,   300] loss: 0.542\n",
      "[2,   400] loss: 0.474\n",
      "[2,   500] loss: 0.442\n",
      "[2,   600] loss: 0.414\n",
      "[2,   700] loss: 0.365\n",
      "[2,   800] loss: 0.330\n",
      "[2,   900] loss: 0.317\n",
      "[2,  1000] loss: 0.311\n",
      "[2,  1100] loss: 0.297\n",
      "[2,  1200] loss: 0.294\n",
      "[3,   100] loss: 0.271\n",
      "[3,   200] loss: 0.261\n",
      "[3,   300] loss: 0.250\n",
      "[3,   400] loss: 0.258\n",
      "[3,   500] loss: 0.228\n",
      "[3,   600] loss: 0.231\n",
      "[3,   700] loss: 0.220\n",
      "[3,   800] loss: 0.209\n",
      "[3,   900] loss: 0.192\n",
      "[3,  1000] loss: 0.210\n",
      "[3,  1100] loss: 0.225\n",
      "[3,  1200] loss: 0.189\n",
      "[4,   100] loss: 0.187\n",
      "[4,   200] loss: 0.183\n",
      "[4,   300] loss: 0.187\n",
      "[4,   400] loss: 0.164\n",
      "[4,   500] loss: 0.182\n",
      "[4,   600] loss: 0.189\n",
      "[4,   700] loss: 0.170\n",
      "[4,   800] loss: 0.155\n",
      "[4,   900] loss: 0.163\n",
      "[4,  1000] loss: 0.172\n",
      "[4,  1100] loss: 0.160\n",
      "[4,  1200] loss: 0.157\n",
      "[5,   100] loss: 0.147\n",
      "[5,   200] loss: 0.149\n",
      "[5,   300] loss: 0.146\n",
      "[5,   400] loss: 0.155\n",
      "[5,   500] loss: 0.155\n",
      "[5,   600] loss: 0.131\n",
      "[5,   700] loss: 0.141\n",
      "[5,   800] loss: 0.130\n",
      "[5,   900] loss: 0.136\n",
      "[5,  1000] loss: 0.130\n",
      "[5,  1100] loss: 0.140\n",
      "[5,  1200] loss: 0.134\n",
      "[6,   100] loss: 0.131\n",
      "[6,   200] loss: 0.130\n",
      "[6,   300] loss: 0.121\n",
      "[6,   400] loss: 0.131\n",
      "[6,   500] loss: 0.119\n",
      "[6,   600] loss: 0.121\n",
      "[6,   700] loss: 0.124\n",
      "[6,   800] loss: 0.123\n",
      "[6,   900] loss: 0.114\n",
      "[6,  1000] loss: 0.118\n",
      "[6,  1100] loss: 0.113\n",
      "[6,  1200] loss: 0.107\n",
      "Finished Training. Final loss = 0.08542836457490921, Total params = 149798\n",
      "Correct = 9710, Total = 10000\n",
      "[955, 0, 3, 0, 1, 3, 5, 1, 3, 3]\n",
      "[1, 1129, 4, 0, 3, 0, 3, 7, 2, 5]\n",
      "[3, 2, 996, 9, 4, 1, 0, 18, 2, 0]\n",
      "[0, 1, 2, 970, 0, 3, 0, 0, 2, 1]\n",
      "[2, 0, 0, 0, 959, 0, 4, 0, 2, 12]\n",
      "[4, 0, 1, 12, 0, 873, 8, 1, 4, 9]\n",
      "[11, 2, 2, 0, 5, 3, 935, 0, 7, 1]\n",
      "[2, 0, 18, 10, 1, 2, 0, 992, 5, 6]\n",
      "[2, 1, 5, 7, 3, 7, 3, 2, 942, 13]\n",
      "[0, 0, 1, 2, 6, 0, 0, 7, 5, 959]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,64,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(64,128,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(128,64,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(64,20,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(20,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root=\"./data\",download = True,train=True,transform=ToTensor())\n",
    "train_loader = DataLoader(mnist_trainset,batch_size=50,shuffle=True)\n",
    "mnist_testset = datasets.MNIST(root=\"./data\",download = True,train=False,transform=ToTensor())\n",
    "test_loader = DataLoader(mnist_testset,batch_size=50,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNClassifier().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "\n",
    "total_params = 0\n",
    "for name,param in model.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "correct,total = 0,0\n",
    "for i,vdata in enumerate(test_loader):\n",
    "    tinputs,tlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "    toutputs = model(tinputs)\n",
    "\n",
    "    _,predicted = torch.max(toutputs,1)\n",
    "    total += tlabels.size(0)\n",
    "    correct += (predicted==tlabels).sum()\n",
    "        \n",
    "print(f\"Correct = {correct}, Total = {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5a7d9-ed64-4d1d-ab72-ed8c580a6cc9",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "Modify CNN of Qn. 3 to reduce the number of parameters in the network. Draw a plot of\n",
    "percentage drop in parameters vs accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50ee7e0-4821-4662-ad21-4f671aa6744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.304\n",
      "[1,   200] loss: 2.294\n",
      "[1,   300] loss: 2.289\n",
      "[1,   400] loss: 2.277\n",
      "[1,   500] loss: 2.259\n",
      "[1,   600] loss: 2.223\n",
      "[1,   700] loss: 2.154\n",
      "[1,   800] loss: 1.979\n",
      "[1,   900] loss: 1.609\n",
      "[1,  1000] loss: 1.187\n",
      "[1,  1100] loss: 0.967\n",
      "[1,  1200] loss: 0.823\n",
      "[2,   100] loss: 0.768\n",
      "[2,   200] loss: 0.636\n",
      "[2,   300] loss: 0.591\n",
      "[2,   400] loss: 0.517\n",
      "[2,   500] loss: 0.480\n",
      "[2,   600] loss: 0.468\n",
      "[2,   700] loss: 0.434\n",
      "[2,   800] loss: 0.423\n",
      "[2,   900] loss: 0.386\n",
      "[2,  1000] loss: 0.374\n",
      "[2,  1100] loss: 0.369\n",
      "[2,  1200] loss: 0.367\n",
      "[3,   100] loss: 0.328\n",
      "[3,   200] loss: 0.341\n",
      "[3,   300] loss: 0.319\n",
      "[3,   400] loss: 0.295\n",
      "[3,   500] loss: 0.295\n",
      "[3,   600] loss: 0.285\n",
      "[3,   700] loss: 0.286\n",
      "[3,   800] loss: 0.284\n",
      "[3,   900] loss: 0.276\n",
      "[3,  1000] loss: 0.283\n",
      "[3,  1100] loss: 0.268\n",
      "[3,  1200] loss: 0.243\n",
      "[4,   100] loss: 0.256\n",
      "[4,   200] loss: 0.236\n",
      "[4,   300] loss: 0.266\n",
      "[4,   400] loss: 0.212\n",
      "[4,   500] loss: 0.251\n",
      "[4,   600] loss: 0.215\n",
      "[4,   700] loss: 0.231\n",
      "[4,   800] loss: 0.234\n",
      "[4,   900] loss: 0.225\n",
      "[4,  1000] loss: 0.195\n",
      "[4,  1100] loss: 0.219\n",
      "[4,  1200] loss: 0.209\n",
      "[5,   100] loss: 0.194\n",
      "[5,   200] loss: 0.215\n",
      "[5,   300] loss: 0.213\n",
      "[5,   400] loss: 0.209\n",
      "[5,   500] loss: 0.183\n",
      "[5,   600] loss: 0.191\n",
      "[5,   700] loss: 0.205\n",
      "[5,   800] loss: 0.203\n",
      "[5,   900] loss: 0.184\n",
      "[5,  1000] loss: 0.175\n",
      "[5,  1100] loss: 0.191\n",
      "[5,  1200] loss: 0.167\n",
      "[6,   100] loss: 0.176\n",
      "[6,   200] loss: 0.174\n",
      "[6,   300] loss: 0.175\n",
      "[6,   400] loss: 0.165\n",
      "[6,   500] loss: 0.183\n",
      "[6,   600] loss: 0.153\n",
      "[6,   700] loss: 0.178\n",
      "[6,   800] loss: 0.167\n",
      "[6,   900] loss: 0.172\n",
      "[6,  1000] loss: 0.170\n",
      "[6,  1100] loss: 0.153\n",
      "[6,  1200] loss: 0.168\n",
      "Finished Training. Final loss = 0.07159193605184555, Total params = 9594\n",
      "Correct = 9531, Total = 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class CNNClassifier1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(16,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(32,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(16,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root=\"./data\",download = True,train=True,transform=ToTensor())\n",
    "train_loader = DataLoader(mnist_trainset,batch_size=50,shuffle=True)\n",
    "mnist_testset = datasets.MNIST(root=\"./data\",download = True,train=False,transform=ToTensor())\n",
    "test_loader = DataLoader(mnist_testset,batch_size=50,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = CNNClassifier1().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "\n",
    "total_params = 0\n",
    "for name,param in model1.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model1(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "correct,total = 0,0\n",
    "for i,vdata in enumerate(test_loader):\n",
    "    tinputs,tlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "    toutputs = model1(tinputs)\n",
    "\n",
    "    _,predicted = torch.max(toutputs,1)\n",
    "    total += tlabels.size(0)\n",
    "    correct += (predicted==tlabels).sum()\n",
    "        \n",
    "print(f\"Correct = {correct}, Total = {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c000d6-9bbc-4796-bf88-f31189c20dc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.310\n",
      "[1,   200] loss: 2.305\n",
      "[1,   300] loss: 2.300\n",
      "[1,   400] loss: 2.298\n",
      "[1,   500] loss: 2.293\n",
      "[1,   600] loss: 2.289\n",
      "[1,   700] loss: 2.281\n",
      "[1,   800] loss: 2.270\n",
      "[1,   900] loss: 2.251\n",
      "[1,  1000] loss: 2.209\n",
      "[1,  1100] loss: 2.121\n",
      "[1,  1200] loss: 1.938\n",
      "[2,   100] loss: 1.663\n",
      "[2,   200] loss: 1.413\n",
      "[2,   300] loss: 1.158\n",
      "[2,   400] loss: 0.963\n",
      "[2,   500] loss: 0.842\n",
      "[2,   600] loss: 0.694\n",
      "[2,   700] loss: 0.601\n",
      "[2,   800] loss: 0.546\n",
      "[2,   900] loss: 0.471\n",
      "[2,  1000] loss: 0.427\n",
      "[2,  1100] loss: 0.408\n",
      "[2,  1200] loss: 0.381\n",
      "[3,   100] loss: 0.359\n",
      "[3,   200] loss: 0.318\n",
      "[3,   300] loss: 0.313\n",
      "[3,   400] loss: 0.297\n",
      "[3,   500] loss: 0.277\n",
      "[3,   600] loss: 0.285\n",
      "[3,   700] loss: 0.257\n",
      "[3,   800] loss: 0.243\n",
      "[3,   900] loss: 0.223\n",
      "[3,  1000] loss: 0.252\n",
      "[3,  1100] loss: 0.238\n",
      "[3,  1200] loss: 0.229\n",
      "[4,   100] loss: 0.217\n",
      "[4,   200] loss: 0.208\n",
      "[4,   300] loss: 0.231\n",
      "[4,   400] loss: 0.195\n",
      "[4,   500] loss: 0.194\n",
      "[4,   600] loss: 0.214\n",
      "[4,   700] loss: 0.177\n",
      "[4,   800] loss: 0.194\n",
      "[4,   900] loss: 0.181\n",
      "[4,  1000] loss: 0.178\n",
      "[4,  1100] loss: 0.171\n",
      "[4,  1200] loss: 0.172\n",
      "[5,   100] loss: 0.172\n",
      "[5,   200] loss: 0.166\n",
      "[5,   300] loss: 0.168\n",
      "[5,   400] loss: 0.154\n",
      "[5,   500] loss: 0.155\n",
      "[5,   600] loss: 0.165\n",
      "[5,   700] loss: 0.158\n",
      "[5,   800] loss: 0.157\n",
      "[5,   900] loss: 0.142\n",
      "[5,  1000] loss: 0.159\n",
      "[5,  1100] loss: 0.161\n",
      "[5,  1200] loss: 0.133\n",
      "[6,   100] loss: 0.161\n",
      "[6,   200] loss: 0.141\n",
      "[6,   300] loss: 0.149\n",
      "[6,   400] loss: 0.131\n",
      "[6,   500] loss: 0.144\n",
      "[6,   600] loss: 0.137\n",
      "[6,   700] loss: 0.121\n",
      "[6,   800] loss: 0.127\n",
      "[6,   900] loss: 0.133\n",
      "[6,  1000] loss: 0.130\n",
      "[6,  1100] loss: 0.117\n",
      "[6,  1200] loss: 0.128\n",
      "Finished Training. Final loss = 0.2579786479473114, Total params = 38150\n",
      "[1,   100] loss: 2.303\n",
      "[1,   200] loss: 2.300\n",
      "[1,   300] loss: 2.295\n",
      "[1,   400] loss: 2.292\n",
      "[1,   500] loss: 2.288\n",
      "[1,   600] loss: 2.278\n",
      "[1,   700] loss: 2.267\n",
      "[1,   800] loss: 2.245\n",
      "[1,   900] loss: 2.198\n",
      "[1,  1000] loss: 2.097\n",
      "[1,  1100] loss: 1.952\n",
      "[1,  1200] loss: 1.809\n",
      "[2,   100] loss: 1.676\n",
      "[2,   200] loss: 1.588\n",
      "[2,   300] loss: 1.453\n",
      "[2,   400] loss: 1.226\n",
      "[2,   500] loss: 1.036\n",
      "[2,   600] loss: 0.859\n",
      "[2,   700] loss: 0.686\n",
      "[2,   800] loss: 0.616\n",
      "[2,   900] loss: 0.522\n",
      "[2,  1000] loss: 0.466\n",
      "[2,  1100] loss: 0.419\n",
      "[2,  1200] loss: 0.395\n",
      "[3,   100] loss: 0.341\n",
      "[3,   200] loss: 0.323\n",
      "[3,   300] loss: 0.292\n",
      "[3,   400] loss: 0.291\n",
      "[3,   500] loss: 0.277\n",
      "[3,   600] loss: 0.280\n",
      "[3,   700] loss: 0.242\n",
      "[3,   800] loss: 0.229\n",
      "[3,   900] loss: 0.231\n",
      "[3,  1000] loss: 0.239\n",
      "[3,  1100] loss: 0.222\n",
      "[3,  1200] loss: 0.216\n",
      "[4,   100] loss: 0.191\n",
      "[4,   200] loss: 0.215\n",
      "[4,   300] loss: 0.172\n",
      "[4,   400] loss: 0.181\n",
      "[4,   500] loss: 0.178\n",
      "[4,   600] loss: 0.179\n",
      "[4,   700] loss: 0.166\n",
      "[4,   800] loss: 0.158\n",
      "[4,   900] loss: 0.169\n",
      "[4,  1000] loss: 0.161\n",
      "[4,  1100] loss: 0.178\n",
      "[4,  1200] loss: 0.144\n",
      "[5,   100] loss: 0.158\n",
      "[5,   200] loss: 0.128\n",
      "[5,   300] loss: 0.143\n",
      "[5,   400] loss: 0.143\n",
      "[5,   500] loss: 0.140\n",
      "[5,   600] loss: 0.145\n",
      "[5,   700] loss: 0.128\n",
      "[5,   800] loss: 0.141\n",
      "[5,   900] loss: 0.138\n",
      "[5,  1000] loss: 0.117\n",
      "[5,  1100] loss: 0.120\n",
      "[5,  1200] loss: 0.131\n",
      "[6,   100] loss: 0.124\n",
      "[6,   200] loss: 0.108\n",
      "[6,   300] loss: 0.113\n",
      "[6,   400] loss: 0.114\n",
      "[6,   500] loss: 0.121\n",
      "[6,   600] loss: 0.128\n",
      "[6,   700] loss: 0.098\n",
      "[6,   800] loss: 0.112\n",
      "[6,   900] loss: 0.097\n",
      "[6,  1000] loss: 0.116\n",
      "[6,  1100] loss: 0.110\n",
      "[6,  1200] loss: 0.112\n",
      "Finished Training. Final loss = 0.055033281445503235, Total params = 601254\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root=\"./data\",download = True,train=True,transform=ToTensor())\n",
    "train_loader = DataLoader(mnist_trainset,batch_size=50,shuffle=True)\n",
    "mnist_testset = datasets.MNIST(root=\"./data\",download = True,train=False,transform=ToTensor())\n",
    "test_loader = DataLoader(mnist_testset,batch_size=50,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class CNNClassifier1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,128,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(128,256,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(256,128,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(128,64,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(64,20,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(20,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "class CNNClassifier2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(32,64,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(64,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(32,20,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(20,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "    \n",
    "class CNNClassifier3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(16,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(32,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(16,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "model1 = CNNClassifier1().to(device)\n",
    "model2 = CNNClassifier2().to(device)\n",
    "model3 = CNNClassifier3().to(device)\n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "loss = None\n",
    "total_params = 0\n",
    "\n",
    "for name,param in model1.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model1(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "loss = None\n",
    "total_params = 0\n",
    "\n",
    "for name,param in model2.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model2(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "\n",
    "loss = None\n",
    "optimizer = optim.SGD(model3.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "\n",
    "total_params = 0\n",
    "for name,param in model3.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model3(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfeb891a-6f7a-4bf5-b9e2-5898f7f63ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21acdb72700>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = [0.055033281445503235,0.08542836457490921,0.2579786479473114,0.07159193605184555]\n",
    "params = [601254,149798,38150,9594]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(params,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0587c0-b4d4-476a-8b7c-d9e6a528346a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
