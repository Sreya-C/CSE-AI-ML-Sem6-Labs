{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1f5074-7ca2-42ad-9df4-5ca8e98384bb",
   "metadata": {},
   "source": [
    "## SOLVED EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad2feb3b-6718-470d-b163-70dfd71bf2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Solution tensor([-2.])\n",
      "Analytical Solution -2\n",
      "Autograd:  0.10499356687068939\n",
      "Manual:  0.1049935854035065\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#1 f(x) = (x-2)^2. Compute f'(1)\n",
    "\n",
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "def fp(x):\n",
    "    return 2*(x-2)\n",
    "\n",
    "x = torch.tensor([1.0],requires_grad = True)\n",
    "y = f(x)\n",
    "y.backward()\n",
    "\n",
    "print(\"Pytorch Solution\",x.grad)\n",
    "print(\"Analytical Solution\",fp(1))\n",
    "\n",
    "#Compute gradient of the sigmoid function\n",
    "def grad_manual(x):\n",
    "    a = -x\n",
    "    b = np.exp(a)\n",
    "    c = 1+b\n",
    "    e = 1.0/c\n",
    "\n",
    "    dedc = -1.0/(c**2)\n",
    "    dedb = dedc*1\n",
    "    deda = dedb*np.exp(a)\n",
    "    dedx = deda*(-1)\n",
    "\n",
    "    return dedx\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + torch.exp(-x))\n",
    "\n",
    "inp_x = 2.0\n",
    "x = torch.tensor(inp_x,requires_grad = True)\n",
    "y = sigmoid(x)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"Autograd: \",x.grad.item())\n",
    "print(\"Manual: \",grad_manual(inp_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fe799-dde0-48a6-b118-238aa7d8a6c8",
   "metadata": {},
   "source": [
    "## Q1. Compute dz/da and compare with analytical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a1230b7-828b-4e4d-9932-1dba9cd5f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor(8., grad_fn=<AddBackward0>)\n",
      "Y tensor(29., grad_fn=<AddBackward0>)\n",
      "Z tensor(103., grad_fn=<AddBackward0>)\n",
      "Gradient dz/da at a=1 and b=2 is tensor(34.)\n",
      "Gradient dz/db at a=1 and b=2 is tensor(114.)\n",
      "Manual Solution at a=1 and b=2 is (34.0, 114.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inp_a = 1.0\n",
    "inp_b = 2.0\n",
    "a = torch.tensor(1.0,requires_grad = True)\n",
    "b = torch.tensor(2.0,requires_grad = True)\n",
    "\n",
    "x = 2*a + 3*b\n",
    "y = 5*a*a + 3*b*b*b\n",
    "z = 2*x + 3*y\n",
    "\n",
    "def manual(a,b):\n",
    "    x = 2*a + 3*b\n",
    "    y = 5*a*a + 3*b*b*b\n",
    "    z = 2*x + 3*y\n",
    "    \n",
    "    dzda = 2*2 + 3*5*2*a\n",
    "    dzdb = 2*3*1 + 3*3*3*b*b\n",
    "\n",
    "    return (dzda,dzdb)\n",
    "\n",
    "print(\"X\",x)\n",
    "print(\"Y\",y)\n",
    "print(\"Z\",z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"Gradient dz/da at a=1 and b=2 is\", a.grad)\n",
    "print(\"Gradient dz/db at a=1 and b=2 is\", b.grad)\n",
    "\n",
    "print(\"Manual Solution at a=1 and b=2 is\",manual(inp_a,inp_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7332eb1-2068-430c-b0ba-9ceedfdccd92",
   "metadata": {},
   "source": [
    "## Q2. Work out da/dw and compare with analytical gradient (ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c96f7d9-110b-44c4-b6ba-2f3f647c5804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(1.)\n",
      "3.0 1.0\n"
     ]
    }
   ],
   "source": [
    "def manual(w,b,x):\n",
    "    u = w*x\n",
    "    v = u + b\n",
    "    a = max(0.0,v)\n",
    "\n",
    "    if a == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        dadv = 1.\n",
    "        dadu = 1.\n",
    "        dadb = 1.\n",
    "        dadw = x\n",
    "        return dadw,dadb\n",
    "\n",
    "inp_x = 3.0\n",
    "inp_w = 4.0\n",
    "inp_b = 5.0\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "w = torch.tensor(4.0,requires_grad = True)\n",
    "b = torch.tensor(5.0,requires_grad = True)\n",
    "\n",
    "u = w*x;u.retain_grad()\n",
    "v = u + b;v.retain_grad()\n",
    "relu = torch.nn.ReLU();\n",
    "a = relu(v);a.retain_grad()\n",
    "\n",
    "a.backward(retain_graph = False)\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "grad_w,grad_b = manual(inp_w,inp_b,inp_x)\n",
    "print(grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65bf646-bad0-4aa5-b277-f2b589130962",
   "metadata": {},
   "source": [
    "## Q3. Work out da/dw and compare with analytical gradient (Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d5cd597-fa04-4c25-8674-093e68b41944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0123)\n",
      "tensor(0.0025)\n",
      "0.012332546456800243 0.0024665092913600485\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "def sigmoid_grad_manual(x):\n",
    "    a = -x\n",
    "    b = np.exp(a)\n",
    "    c = 1+b\n",
    "    e = 1.0/c\n",
    "\n",
    "    dedc = -1.0/(c**2)\n",
    "    dedb = dedc*1\n",
    "    deda = dedb*np.exp(a)\n",
    "    dedx = deda*(-1)\n",
    "\n",
    "    return dedx\n",
    "\n",
    "def manual(w,b,x):\n",
    "    u = w*x\n",
    "    v = u + b\n",
    "    a = sigmoid(v)\n",
    "\n",
    "    dadv = sigmoid_grad_manual(v)\n",
    "    dadu = dadv*1\n",
    "    dadb = dadv*1\n",
    "    dadw = dadu*x\n",
    "    return dadw,dadb\n",
    "\n",
    "\n",
    "inp_x = 5.0\n",
    "inp_w = 1.0\n",
    "inp_b = 1.0\n",
    "\n",
    "x = torch.tensor(inp_x)\n",
    "w = torch.tensor(inp_w,requires_grad = True)\n",
    "b = torch.tensor(inp_b,requires_grad = True)\n",
    "\n",
    "u = w*x\n",
    "v = u + b\n",
    "sigmoidal = torch.nn.Sigmoid()\n",
    "a = sigmoidal(v)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "grad_w,grad_b = manual(inp_w,inp_b,inp_x)\n",
    "print(grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d09453-f635-4090-b1be-e7bfaf263799",
   "metadata": {},
   "source": [
    "## Q4. f = exp(-x^2 - 2x - sin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bccf798b-acb2-4f70-8dd7-c5803c6bba27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Solution: tensor(-0.0008)\n",
      "Analytical Solution: -0.0007545278582400674\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.exp((-x**2)-2*x-np.sin(x))\n",
    "\n",
    "def torchf(x):\n",
    "    return torch.exp((-x**2)-2*x-torch.sin(x))\n",
    "    \n",
    "def fdiff(x):\n",
    "    return f(x)*((-2*x)-2-np.cos(x))\n",
    "\n",
    "inp_x = 2.0\n",
    "x = torch.tensor(inp_x,requires_grad = True)\n",
    "y = torchf(x)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"Pytorch Solution:\",x.grad)\n",
    "print(\"Analytical Solution:\",fdiff(inp_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea3041-7371-4393-aeee-fd7e0a6626ee",
   "metadata": {},
   "source": [
    "## Q5. y = 8x^4 + 3x^3 + 7x^2 + 6x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58d1443d-2d41-4b9c-972f-59317c647765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Solution tensor(326.)\n",
      "Analytical Solution 326.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def f(x):\n",
    "    return 8*(x**4) + 3*(x**3) + 7*(x**2) + 6*x + 3\n",
    "\n",
    "def fdiff(x):\n",
    "    return 32*(x**3) + 9*(x**2) + 14*x + 6\n",
    "\n",
    "inp_x = 2.0\n",
    "x = torch.tensor(inp_x,requires_grad = True)\n",
    "y = f(x)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"Pytorch Solution\",x.grad)\n",
    "print(\"Analytical Solution\",fdiff(inp_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37ba31-9168-4723-b926-f37174ba879e",
   "metadata": {},
   "source": [
    "## Q6. f(x,y,z) = tanh(ln(1 + (z*2x)/sin(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf9538-f04c-48d7-9262-001a15fa1f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
