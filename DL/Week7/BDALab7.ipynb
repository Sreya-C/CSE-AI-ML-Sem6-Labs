{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define our own column names. Then, we show all possible labels in decreasing order of their count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|           label| count|\n",
      "+----------------+------+\n",
      "|          smurf.|280790|\n",
      "|        neptune.|107201|\n",
      "|         normal.| 97278|\n",
      "|           back.|  2203|\n",
      "|          satan.|  1589|\n",
      "|        ipsweep.|  1247|\n",
      "|      portsweep.|  1040|\n",
      "|    warezclient.|  1020|\n",
      "|       teardrop.|   979|\n",
      "|            pod.|   264|\n",
      "|           nmap.|   231|\n",
      "|   guess_passwd.|    53|\n",
      "|buffer_overflow.|    30|\n",
      "|           land.|    21|\n",
      "|    warezmaster.|    20|\n",
      "|           imap.|    12|\n",
      "|        rootkit.|    10|\n",
      "|     loadmodule.|     9|\n",
      "|      ftp_write.|     8|\n",
      "|       multihop.|     7|\n",
      "+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_without_header = spark.read.option(\"inferSchema\",\"True\").option(\"header\",\"False\").csv(\"data/kddcup.data_10_percent_corrected\")\n",
    "\n",
    "column_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\n",
    "                \"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "                \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\n",
    "                \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "                \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\n",
    "                \"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
    "                \"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
    "\n",
    "data = data_without_header.toDF(*column_names)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to Vector which is acceptable as an input for the KMeans Model using Vector Assembler. It is then fit to the KMeans model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineModel_604d930b5970\n",
      "[array([4.79793956e+01, 1.62207883e+03, 8.68534183e+02, 4.45326100e-05,\n",
      "       6.43293794e-03, 1.41694668e-05, 3.45168212e-02, 1.51815716e-04,\n",
      "       1.48247035e-01, 1.02121372e-02, 1.11331525e-04, 3.64357718e-05,\n",
      "       1.13517671e-02, 1.08295211e-03, 1.09307315e-04, 1.00805635e-03,\n",
      "       0.00000000e+00, 0.00000000e+00, 1.38658354e-03, 3.32286248e+02,\n",
      "       2.92907143e+02, 1.76685418e-01, 1.76607809e-01, 5.74330999e-02,\n",
      "       5.77183920e-02, 7.91548844e-01, 2.09816404e-02, 2.89968625e-02,\n",
      "       2.32470732e+02, 1.88666046e+02, 7.53781203e-01, 3.09056111e-02,\n",
      "       6.01935529e-01, 6.68351484e-03, 1.76753957e-01, 1.76441622e-01,\n",
      "       5.81176268e-02, 5.74111170e-02]),\n",
      " array([2.0000000e+00, 6.9337564e+08, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.7000000e+01,\n",
      "       3.0000000e+00, 7.9000000e-01, 6.7000000e-01, 2.1000000e-01,\n",
      "       3.3000000e-01, 5.0000000e-02, 3.9000000e-01, 0.0000000e+00,\n",
      "       2.5500000e+02, 3.0000000e+00, 1.0000000e-02, 9.0000000e-02,\n",
      "       2.2000000e-01, 0.0000000e+00, 1.8000000e-01, 6.7000000e-01,\n",
      "       5.0000000e-02, 3.3000000e-01])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans,KMeansModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "numeric_only = data.drop(\"protocol_type\",\"service\",\"flag\").cache()\n",
    "\n",
    "assembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "\n",
    "kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler,kmeans])\n",
    "pipeline_model = pipeline.fit(numeric_only)\n",
    "print(pipeline_model)\n",
    "kmeans_model = pipeline_model.stages[1]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(kmeans_model.clusterCenters()) #We see that there are 2 cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking which cluster each label belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+\n",
      "|cluster|           label| count|\n",
      "+-------+----------------+------+\n",
      "|      0|          smurf.|280790|\n",
      "|      0|        neptune.|107201|\n",
      "|      0|         normal.| 97278|\n",
      "|      0|           back.|  2203|\n",
      "|      0|          satan.|  1589|\n",
      "|      0|        ipsweep.|  1247|\n",
      "|      0|      portsweep.|  1039|\n",
      "|      0|    warezclient.|  1020|\n",
      "|      0|       teardrop.|   979|\n",
      "|      0|            pod.|   264|\n",
      "|      0|           nmap.|   231|\n",
      "|      0|   guess_passwd.|    53|\n",
      "|      0|buffer_overflow.|    30|\n",
      "|      0|           land.|    21|\n",
      "|      0|    warezmaster.|    20|\n",
      "|      0|           imap.|    12|\n",
      "|      0|        rootkit.|    10|\n",
      "|      0|     loadmodule.|     9|\n",
      "|      0|      ftp_write.|     8|\n",
      "|      0|       multihop.|     7|\n",
      "|      0|            phf.|     4|\n",
      "|      0|           perl.|     3|\n",
      "|      0|            spy.|     2|\n",
      "|      1|      portsweep.|     1|\n",
      "+-------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_cluster = pipeline_model.transform(numeric_only)\n",
    "\n",
    "with_cluster.select(\"cluster\",\"label\").groupBy(\"cluster\",\"label\").count().\\\n",
    "            orderBy(col(\"cluster\"),col(\"count\").desc()).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a good k value by checking the training cost with different values of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34526681198781.35\n",
      "29665347132096.52\n",
      "7464324642470.006\n",
      "5140760167652.76\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from random import randint\n",
    "\n",
    "def clustering_score(input_data,k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\",\"service\",\"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler,kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(20,100,20)):\n",
    "    print(clustering_score(numeric_only,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 34526683484566.53\n",
      "40 13451819211364.23\n",
      "60 15431179848879.225\n",
      "80 1010315645827.3057\n",
      "100 6585306634183.479\n"
     ]
    }
   ],
   "source": [
    "def clustering_score1(input_data,k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\",\"service\",\"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).\\\n",
    "                setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler,kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(20,101,20)):\n",
    "    print(k, clustering_score1(numeric_only,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Normalization - Scale the output vector using Standard Scaler. We observe a lower training cost after the Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 590415.1909242754\n",
      "90 323029.990531517\n",
      "120 239660.47827700234\n",
      "150 182955.6791634747\n",
      "180 149045.53049408298\n",
      "210 127174.45940500751\n",
      "240 112796.81047590676\n",
      "270 103331.46127469104\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "def clustering_score2(input_data,k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\",\"service\",\"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").\\\n",
    "                setWithStd(True).setWithMean(False)\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).\\\n",
    "                setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler,scaler,kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in range(60,271,30):\n",
    "    print(k,clustering_score2(numeric_only,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Categorical Features using One-Hot Encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 16871919.58534714\n",
      "90 6957457.837591373\n",
      "120 1497204.954539814\n",
      "150 1098811.7808264522\n",
      "180 766537.3279262196\n",
      "210 586282.7890263062\n",
      "240 477041.19137717434\n",
      "270 443718.55476593855\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "def one_hot_pipeline(input_col):\n",
    "    indexer = StringIndexer().setInputCol(input_col).setOutputCol(input_col+\"_indexed\")\n",
    "    encoder = OneHotEncoder().setInputCol(input_col+\"_indexed\").setOutputCol(input_col+\"_vec\")\n",
    "    pipeline = Pipeline().setStages([indexer,encoder])\n",
    "    return pipeline,input_col+\"_vec\"\n",
    "\n",
    "def clustering_score3(input_data,k):\n",
    "    proto_type_pipeline,proto_type_vec_col = one_hot_pipeline(\"protocol_type\")\n",
    "    service_pipeline,service_vec_col = one_hot_pipeline(\"service\")\n",
    "    flag_pipeline,flag_vec_col = one_hot_pipeline(\"flag\")\n",
    "    assemble_cols = set(input_data.columns) - {\"label\",\"protocol_type\",\"service\",\"flag\"} | \\\n",
    "                    {proto_type_vec_col,service_vec_col,flag_vec_col}\n",
    "    assembler = VectorAssembler().setInputCols(list(assemble_cols)).setOutputCol(\"featureVector\")\n",
    "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").\\\n",
    "                setWithStd(True).setWithMean(False)\n",
    "    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).\\\n",
    "                setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
    "    pipeline = Pipeline().setStages([proto_type_pipeline,service_pipeline,flag_pipeline,assembler,scaler,kmeans])\n",
    "    pipeline_model = pipeline.fit(input_data)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in range(60,271,30):\n",
    "    print(k,clustering_score3(data,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using labels with entropy. The dataframe now contains - cluster, label, count and probability of that label occuring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+--------------------+\n",
      "|cluster|           label| count|               p_col|\n",
      "+-------+----------------+------+--------------------+\n",
      "|      0|        ipsweep.|  1247|0.002524189304076758|\n",
      "|      0|       teardrop.|   979|0.001981701145702603|\n",
      "|      0|buffer_overflow.|    30|6.072628638516659E-5|\n",
      "|      0|          smurf.|280790|  0.5683777984696976|\n",
      "|      0|        neptune.|107201| 0.21699728755920814|\n",
      "|      0|   guess_passwd.|    53|1.072831059471276...|\n",
      "|      0|         normal.| 97278| 0.19691105623254118|\n",
      "|      0|           perl.|     3| 6.07262863851666E-6|\n",
      "|      0|     loadmodule.|     9|1.821788591554997...|\n",
      "|      0|      portsweep.|  1039|0.002103153718472...|\n",
      "|      0|            pod.|   264| 5.34391320189466E-4|\n",
      "|      0|       multihop.|     7|1.416946682320553...|\n",
      "|      0|          satan.|  1589|0.003216468968867657|\n",
      "|      0|           nmap.|   231|4.675924051657828E-4|\n",
      "|      0|           back.|  2203|  0.0044593336302174|\n",
      "|      0|           land.|    21|4.250840046961661...|\n",
      "|      0|            phf.|     4|8.096838184688879E-6|\n",
      "|      0|           imap.|    12|2.429051455406664E-5|\n",
      "|      0|      ftp_write.|     8|1.619367636937775...|\n",
      "|      0|    warezmaster.|    20|4.048419092344439E-5|\n",
      "+-------+----------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.557605039016584"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Window\n",
    "\n",
    "def entropy(counts):\n",
    "    values = [c for c in counts if c>0]\n",
    "    n = sum(values)\n",
    "    p = [v/n for v in values]\n",
    "    return sum([-1*(p_v) * log(p_v) for p_v in p])\n",
    "\n",
    "cluster_label = pipeline_model.transform(data).select(\"cluster\",\"label\")\n",
    "df = cluster_label.groupBy(\"cluster\",\"label\").count().orderBy(\"cluster\")\n",
    "w = Window.partitionBy(\"cluster\")\n",
    "p_col = df[\"count\"]/f.sum(df[\"count\"]).over(w)\n",
    "with_p_col = df.withColumn(\"p_col\",p_col)\n",
    "with_p_col.show()\n",
    "\n",
    "result = with_p_col.groupBy(\"cluster\").agg((-f.sum(col(\"p_col\")*f.log2(col(\"p_col\")))).alias(\"entropy\"),\n",
    "                f.sum(col(\"count\")).alias(\"cluster_size\"))\n",
    "result = result.withColumn(\"weightedClusterEntropy\",f.col(\"entropy\")*f.col(\"cluster_size\"))\n",
    "weighted_cluster_entropy_avg = result.agg(f.sum(col(\"weightedClusterEntropy\"))).collect()\n",
    "weighted_cluster_entropy_avg[0][0]/data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|cluster|     label| count|\n",
      "+-------+----------+------+\n",
      "|      0|  neptune.| 36502|\n",
      "|      0|portsweep.|    10|\n",
      "|      1|  ipsweep.|     4|\n",
      "|      1|     nmap.|     1|\n",
      "|      1|   normal.|   337|\n",
      "|      1|portsweep.|     1|\n",
      "|      1|    smurf.|280787|\n",
      "|      2|  ipsweep.|     3|\n",
      "|      2|  neptune.|   112|\n",
      "|      2|portsweep.|     1|\n",
      "|      2|    satan.|     1|\n",
      "|      3|  neptune.|    86|\n",
      "|      3|    satan.|     1|\n",
      "|      4|  neptune.|    89|\n",
      "|      5|  neptune.|   106|\n",
      "|      6|  neptune.|    99|\n",
      "|      7|  neptune.|    24|\n",
      "|      7|portsweep.|     3|\n",
      "|      8|  neptune.|   101|\n",
      "|      9|  neptune.|    82|\n",
      "+-------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fit_pipeline_4(data,k):\n",
    "    proto_type_pipeline,proto_type_vec_col = one_hot_pipeline(\"protocol_type\")\n",
    "    service_pipeline,service_vec_col = one_hot_pipeline(\"service\")\n",
    "    flag_pipeline,flag_vec_col = one_hot_pipeline(\"flag\")\n",
    "    assemble_cols = set(data.columns) - {\"label\",\"protocol_type\",\"service\",\"flag\"} | \\\n",
    "                    {proto_type_vec_col,service_vec_col,flag_vec_col}\n",
    "    assembler = VectorAssembler(inputCols=list(assemble_cols),outputCol=\"featureVector\")\n",
    "    scaler = StandardScaler(inputCol=\"featureVector\",outputCol=\"scaledFeatureVector\")\n",
    "    kmeans = KMeans(seed=randint(100,100000),k=k,predictionCol=\"cluster\",featuresCol=\"scaledFeatureVector\",maxIter=40,tol=1.0e-5)\n",
    "    pipeline = Pipeline(stages=[proto_type_pipeline,service_pipeline,flag_pipeline,assembler,scaler,kmeans])\n",
    "    return pipeline.fit(data)\n",
    "\n",
    "# def clusering_score_4(input_data,k): \n",
    "#     pipeline_model = fit_pipeline_4(input_data,k)\n",
    "#     cluster_label = transform(input_data).select(\"cluster\",\"label\")\n",
    "#     df = cluster_label.groupBy(\"cluster\",\"label\").count().orderBy(\"cluster\")\n",
    "#     w = Window.partitionBy(\"cluster\")\n",
    "#     p_col = df[\"count\"]/f.sum(df[\"count\"]).over(w)\n",
    "#     with_p_col = df.withColumn(\"p_col\",p_col)\n",
    "#     with_p_col.show()\n",
    "\n",
    "#     result = with_p_col.groupBy(\"cluster\").agg((-f.sum(col(\"p_col\")*f.log2(col(\"p_col\")))).alias(\"entropy\"),\n",
    "#                     f.sum(col(\"count\")).alias(\"cluster_size\"))\n",
    "#     result = result.withColumn(\"weightedClusterEntropy\",f.col(\"entropy\")*f.col(\"cluster_size\"))\n",
    "#     weighted_cluster_entropy_avg = result.agg(f.sum(col(\"weightedClusterEntropy\"))).collect()\n",
    "#     return  weighted_cluster_entropy_avg[0][0]/data.count()\n",
    "\n",
    "pipeline_model = fit_pipeline_4(data,180)\n",
    "count_by_cluster_label = pipeline_model.transform(data).select(\"cluster\",\"label\").groupBy(\"cluster\",\"label\").count().orderBy(\"cluster\",\"label\")\n",
    "count_by_cluster_label.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
