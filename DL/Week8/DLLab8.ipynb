{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2cf2f-d1bd-49eb-bd41-b2f8c200808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5479\n",
      "1.05 18.48\n",
      "0 th iteration :  tensor(0.1675, grad_fn=<MseLossBackward0>)\n",
      "50 th iteration :  tensor(0.0189, grad_fn=<MseLossBackward0>)\n",
      "100 th iteration :  tensor(0.0221, grad_fn=<MseLossBackward0>)\n",
      "150 th iteration :  tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "200 th iteration :  tensor(0.0171, grad_fn=<MseLossBackward0>)\n",
      "250 th iteration :  tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "300 th iteration :  tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "350 th iteration :  tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
      "400 th iteration :  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "450 th iteration :  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "500 th iteration :  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "550 th iteration :  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "600 th iteration :  tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "650 th iteration :  tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "700 th iteration :  tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "750 th iteration :  tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "800 th iteration :  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "850 th iteration :  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "900 th iteration :  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "950 th iteration :  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "1000 th iteration :  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "1050 th iteration :  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "1100 th iteration :  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "1150 th iteration :  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "1200 th iteration :  tensor(0.0007, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "\n",
    "df = pd.read_csv(\"./data/naturalgas/daily.csv\")\n",
    "\n",
    "# Preprocess the data - Drop NA values in the dataset\n",
    "df = df.dropna()\n",
    "y = df['Price'].values\n",
    "x = np.arange(1, len(y), 1)\n",
    "print(len(y))\n",
    "\n",
    "# Normalize the input range between 0 and 1\n",
    "minm = y.min()\n",
    "maxm = y.max()\n",
    "print(minm, maxm)\n",
    "y = (y - minm) / (maxm - minm)\n",
    "X = []\n",
    "Y = []\n",
    "Sequence_Length = 10\n",
    "\n",
    "# Iterate through the data to create input-output pairs\n",
    "for i in range(0, len(y) - Sequence_Length - 1):  # Adjust loop bounds\n",
    "    x_seq = []\n",
    "    for j in range(i, i + Sequence_Length):\n",
    "        x_seq.append(y[j])\n",
    "    X.append(x_seq)\n",
    "    Y.append(y[i + Sequence_Length])\n",
    "\n",
    "# Convert lists to arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Split the data as the train and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42, shuffle=False, stratify=None)\n",
    "\n",
    "\n",
    "class NGTimeSeries(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.len = x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = NGTimeSeries(x_train, y_train)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset, shuffle=True, batch_size=256)\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=5, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=5, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _status = self.rnn(x)\n",
    "        output = output[:, -1, :]\n",
    "        output = self.fc1(torch.relu(output))\n",
    "        return output\n",
    "\n",
    "\n",
    "model = RNNModel()\n",
    "\n",
    "# optimizer , loss\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "epochs = 1500\n",
    "\n",
    "# training loop\n",
    "for i in range(epochs):\n",
    "    for j, data in enumerate(train_loader):\n",
    "        y_pred = model(data[:][0].view(-1, Sequence_Length,\n",
    "                                       1)).reshape(-1)\n",
    "        loss = criterion(y_pred, data[:][1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if i % 50 == 0:\n",
    "        print(i, \"th iteration : \", loss)\n",
    "\n",
    "# test set actual vs predicted\n",
    "test_set = NGTimeSeries(x_test, y_test)\n",
    "test_pred = model(test_set[:][0].view(-1, 10, 1)).view(-1)\n",
    "plt.plot(test_pred.detach().numpy(), label='predicted')\n",
    "plt.plot(test_set[:][1].view(-1), label='original')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Undo normalization\n",
    "y = y * (maxm - minm) + minm\n",
    "y_pred = test_pred.detach().numpy() * (maxm - minm) + minm\n",
    "plt.plot(y)\n",
    "plt.plot(range(len(y) - len(y_pred), len(y)), y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772795dc-cf13-4da2-beb2-d583ba509a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "# Define a class to load and preprocess the surname data\n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, data_folder, sequence_length=20):\n",
    "        self.languages = {}\n",
    "        self.languages_index = {}\n",
    "        self.languages_list = []\n",
    "        self.surname_data = []\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Load surname data from files\n",
    "        for filename in glob.glob(data_folder + '/*.txt'):\n",
    "            language = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.languages_list.append(language)\n",
    "            self.languages_index[language] = len(self.languages_index)\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                surnames = [line.strip() for line in file]\n",
    "                self.surname_data.extend([(surname, language) for surname in surnames])\n",
    "                self.languages[language] = surnames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.surname_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        surname, language = self.surname_data[idx]\n",
    "        # Convert surname characters to one-hot encoding\n",
    "        surname_tensor = self.surname_to_tensor(surname)\n",
    "        # Convert language to label index\n",
    "        language_label = torch.tensor(self.languages_index[language], dtype=torch.long)\n",
    "        return surname_tensor, language_label\n",
    "\n",
    "    def surname_to_tensor(self, surname):\n",
    "        tensor = torch.zeros(self.sequence_length, len(all_letters))\n",
    "        for i, letter in enumerate(surname):\n",
    "            tensor[i][all_letters.find(letter)] = 1\n",
    "        return tensor\n",
    "\n",
    "\n",
    "# Define the RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout=0.2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :])  # Take the last output only\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "# Define the training function\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for surnames, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(surnames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "\n",
    "# Define function to predict language from surname\n",
    "def predict_language(model, surname):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = dataset.surname_to_tensor(surname).unsqueeze(0)\n",
    "        output = model(input_tensor)\n",
    "        _, predicted_index = torch.max(output, 1)\n",
    "        predicted_language = dataset.languages_list[predicted_index.item()]\n",
    "        return predicted_language\n",
    "\n",
    "\n",
    "# Define a function to randomly sample a surname from the dataset\n",
    "def random_surname(dataset):\n",
    "    surname, language = random.choice(dataset.dataset.surname_data)\n",
    "    return surname\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_folder = 'data/names'\n",
    "all_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "n_languages = 18\n",
    "dataset = SurnameDataset(data_folder)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = len(all_letters)\n",
    "hidden_size = 128\n",
    "output_size = n_languages\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader objects for train and test sets\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = RNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for surnames, labels in test_loader:\n",
    "        outputs = model(surnames)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += 1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Test predictions\n",
    "for i in range(5):\n",
    "    surname = random_surname(test_set)\n",
    "    predicted_language = predict_language(model, surname)\n",
    "    print(surname,predicted_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89dab0f-9008-48c6-997c-428c68078b05",
   "metadata": {},
   "source": [
    "Epoch 1/20, Loss: 1.8792475903651629\\\n",
    "Epoch 2/20, Loss: 1.858789148083721\\\n",
    "Epoch 3/20, Loss: 1.858333488384566\\\n",
    "Epoch 4/20, Loss: 1.8597911287588902\\\n",
    "Epoch 5/20, Loss: 1.8583579809067259\\\n",
    "Epoch 6/20, Loss: 1.8582783790223627\\\n",
    "Epoch 7/20, Loss: 1.8575045487795219\\\n",
    "Epoch 8/20, Loss: 1.8175804463040781\\\n",
    "Epoch 9/20, Loss: 1.7842482945833549\\\n",
    "Epoch 10/20, Loss: 1.8003006043187175\\\n",
    "Epoch 11/20, Loss: 1.7688545373331503\\\n",
    "Epoch 12/20, Loss: 1.75426024032304\\\n",
    "Epoch 13/20, Loss: 1.7383050994569087\\\n",
    "Epoch 14/20, Loss: 1.7576265254343648\\\n",
    "Epoch 15/20, Loss: 1.7648339266796036\\\n",
    "Epoch 16/20, Loss: 1.7362580161645593\\\n",
    "Epoch 17/20, Loss: 1.7887547419840597\\\n",
    "Epoch 18/20, Loss: 1.7784410149927634\\\n",
    "Epoch 19/20, Loss: 1.749617425569025\\\n",
    "Epoch 20/20, Loss: 1.7581511802407375\\\n",
    "Accuracy on test set: 50.11%\\\n",
    "Buxton Russian\\\n",
    "Mingaleev Russian\\\n",
    "Sapienti English\\\n",
    "Nassar English\\\n",
    "Dehant Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8fa25-06fb-4651-8429-3d158e60f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyTextDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        f = open(\"./data/sherlock.txt\", \"r\")\n",
    "        text = f.read().lower().strip()\n",
    "        self.seq_length = 7\n",
    "        self.X, self.y = self.tokens2sequence(text, sequence_length=self.seq_length)\n",
    "        self.vocab = sorted(set(text))\n",
    "        self.chr_to_idx = {chr: i for i, chr in enumerate(self.vocab)}\n",
    "        self.idx_to_chr = {i: chr for chr, i in self.chr_to_idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_idx = [self.chr_to_idx[char] for char in self.X[idx]]\n",
    "        y_idx = self.chr_to_idx[self.y[idx]]\n",
    "        return torch.tensor(X_idx), torch.tensor(y_idx)\n",
    "\n",
    "    def tokens2sequence(self, tokens, sequence_length):\n",
    "        X, y = [], []\n",
    "        for i in range(len(tokens) - sequence_length):\n",
    "            X.append(tokens[i:i + sequence_length])\n",
    "            y.append(tokens[i + sequence_length])\n",
    "        return X, y\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size=32, hidden_size=64):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.rnn = nn.RNN(input_size=self.embedding_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embed(x)\n",
    "        output, _ = self.rnn(embed)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def generate_text(model, initial_text, length, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_text = initial_text.lower()\n",
    "        for _ in range(length):\n",
    "            input_tensor = torch.tensor([[dataset.chr_to_idx[char] for char in generated_text[-7:]]]).to(device)\n",
    "            output = model(input_tensor)\n",
    "            _, predicted_idx = output.max(1)\n",
    "            predicted_char = dataset.idx_to_chr[predicted_idx.item()]\n",
    "            generated_text += predicted_char\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = MyTextDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = RNNModel(vocab_size=len(dataset.vocab))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, dataloader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Generate text using the trained model\n",
    "initial_text = \"Sherlock \"\n",
    "generated_text = generate_text(model, initial_text, length=10, device=device)\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62500d-884f-4073-bd6f-2cbbaf858126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
