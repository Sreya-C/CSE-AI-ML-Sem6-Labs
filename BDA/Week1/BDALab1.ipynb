{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a PySpark program to square set of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "\n",
      "+---+-----------+\n",
      "|  0|int_squared|\n",
      "+---+-----------+\n",
      "|  1|          1|\n",
      "|  2|          4|\n",
      "|  3|          9|\n",
      "|  4|         16|\n",
      "|  5|         25|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "spark = SparkSession.builder.appName(\"square\").getOrCreate()\n",
    "\n",
    "data = [1,2,3,4,5]\n",
    "df_pd = pd.DataFrame(data)\n",
    "df = spark.createDataFrame(df_pd)\n",
    "df.show()\n",
    "\n",
    "@udf\n",
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "df.select('0',square('0').alias('int_squared')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a PySpark program to find the maximum of given set of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "|  9|\n",
      "|  3|\n",
      "| 10|\n",
      "| 34|\n",
      "| 21|\n",
      "|112|\n",
      "|  5|\n",
      "|  2|\n",
      "| 45|\n",
      "+---+\n",
      "\n",
      "Maximum of all integers is: 112\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"maximum\").getOrCreate()\n",
    "\n",
    "df_pd = pd.DataFrame([9,3,10,34,21,112,5,2,45])                                                                                                                                                                                                                                                          \n",
    "df = spark.createDataFrame(df_pd)\n",
    "df.show()\n",
    "\n",
    "print(\"Maximum of all integers is:\",df.select('0').rdd.max()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Write a PySpark program to find average of N numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "|  9|\n",
      "|  3|\n",
      "| 10|\n",
      "| 34|\n",
      "| 21|\n",
      "|112|\n",
      "|  5|\n",
      "|  2|\n",
      "| 45|\n",
      "+---+\n",
      "\n",
      "+-----------------+\n",
      "|          Average|\n",
      "+-----------------+\n",
      "|26.77777777777778|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"avg\").getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [9,3,10,34,21,112,5,2,45]\n",
    "df_pd = pd.DataFrame(data)\n",
    "df = spark.createDataFrame(df_pd)\n",
    "\n",
    "df.show()\n",
    "    \n",
    "df.agg(F.avg(df['0']).alias(\"Average\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Demonstrate how to read a CSV file into a PySpark DataFram\n",
    "\n",
    "Q5: Use PySpark commands to display the first few rows and schema of a DataFrame\n",
    "\n",
    "Q6: Calculate basic summary statistics for a specific column in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: integer (nullable = true)\n",
      " |-- col2: integer (nullable = true)\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|  12|   3|\n",
      "|  13|  45|\n",
      "|  15|  34|\n",
      "|  17|  31|\n",
      "|  46|  76|\n",
      "+----+----+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|              col1|\n",
      "+-------+------------------+\n",
      "|  count|                 5|\n",
      "|   mean|              20.6|\n",
      "| stddev|14.328293687665674|\n",
      "|    min|                12|\n",
      "|    25%|                13|\n",
      "|    50%|                15|\n",
      "|    75%|                17|\n",
      "|    max|                46|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Read CSV\").getOrCreate()\n",
    "sparkcsv = spark.read.csv('sample.csv',header = True, inferSchema = True)\n",
    "\n",
    "sparkcsv.printSchema()\n",
    "sparkcsv.head(3)\n",
    "sparkcsv.show()\n",
    "sparkcsv.select(\"col1\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
